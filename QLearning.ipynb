{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOcGm/kbiUNJ5eF0/s0zZ4O"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Basic dependencies"
      ],
      "metadata": {
        "id": "6tBYPz0vaWiL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jc9gKrkMyLz6"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install pyglet==1.5.1 \n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "\n",
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/openai/gym.git # We install gym using git since Taxi-v3 \"rgb_array version\" is not on PyPi release\n",
        "!pip install pygame\n",
        "!pip install numpy\n",
        "\n",
        "!pip install huggingface_hub\n",
        "!pip install pickle5\n",
        "!pip install pyyaml==6.0 # avoid key error metadata"
      ],
      "metadata": {
        "id": "cVdRalPSZ5e1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "import random\n",
        "import imageio\n",
        "import os\n",
        "\n",
        "import pickle5 as pickle"
      ],
      "metadata": {
        "id": "nK5wnh2aZ83W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xhf4004S8rp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"FrozenLake-v1\", map_name=\"4x4\", is_slippery=False)"
      ],
      "metadata": {
        "id": "FgR0_jhCA6B3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
        "env.reset()\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space\", env.observation_space)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ],
      "metadata": {
        "id": "bYw1Kc55BBxz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37db4acc-e0ca-4266-de22-8db90bd8ddc9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_____OBSERVATION SPACE_____ \n",
            "\n",
            "Observation Space Discrete(16)\n",
            "Sample observation 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ],
      "metadata": {
        "id": "v76mTs0OBFuf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "639e6b52-8d2d-4110-e95d-734e4ab6a1a5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " _____ACTION SPACE_____ \n",
            "\n",
            "Action Space Shape 4\n",
            "Action Space Sample 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "state_space = env.observation_space.n\n",
        "print(\"There are \", state_space, \" possible states\")\n",
        "\n",
        "action_space = env.action_space.n\n",
        "print(\"There are \", action_space, \" possible actions\")"
      ],
      "metadata": {
        "id": "S4EIwtX8BIdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "314c5991-9774-4a3e-8234-f7ad23545f18"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are  16  possible states\n",
            "There are  4  possible actions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create our Qtable of size (state_space, action_space) and initialized each values at 0 using np.zeros\n",
        "def initialize_q_table(state_space, action_space):\n",
        "  Qtable = np.zeros((state_space, action_space))\n",
        "  return Qtable"
      ],
      "metadata": {
        "id": "A2dthKErBM4k"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
      ],
      "metadata": {
        "id": "v93fXbk6BP60"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy_policy(Qtable, state):\n",
        "  # Randomly generate a number between 0 and 1\n",
        "  random_int = random.uniform(0,1)\n",
        "  # if random_int > greater than epsilon --> exploitation\n",
        "  if random_int > epsilon:\n",
        "    # Take the action with the highest value given a state\n",
        "    # np.argmax can be useful here\n",
        "    action = np.argmax(Qtable[state])\n",
        "  # else --> exploration\n",
        "  else:\n",
        "    action = env.action_space.sample()\n",
        "  \n",
        "  return action"
      ],
      "metadata": {
        "id": "A-dvJss074-q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def greedy_policy(Qtable, state):\n",
        "  # Exploitation: take the action with the highest state, action value\n",
        "  action = np.argmax(Qtable[state])\n",
        "  \n",
        "  return action"
      ],
      "metadata": {
        "id": "WzPO-Cqv76A-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "n_training_episodes = 10000  # Total training episodes\n",
        "learning_rate = 0.7          # Learning rate\n",
        "\n",
        "# Evaluation parameters\n",
        "n_eval_episodes = 100        # Total number of test episodes\n",
        "\n",
        "# Environment parameters\n",
        "env_id = \"FrozenLake-v1\"     # Name of the environment\n",
        "max_steps = 99               # Max steps per episode\n",
        "gamma = 0.95                 # Discounting rate\n",
        "eval_seed = []               # The evaluation seed of the environment\n",
        "\n",
        "# Exploration parameters\n",
        "epsilon = 1.0                 # Exploration rate\n",
        "max_epsilon = 1.0             # Exploration probability at start\n",
        "min_epsilon = 0.05            # Minimum exploration probability \n",
        "decay_rate = 0.005            # Exponential decay rate for exploration prob"
      ],
      "metadata": {
        "id": "mUzEXHi578x1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
        "  for episode in range(n_training_episodes):\n",
        "    # Reduce epsilon (because we need less and less exploration)\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n",
        "    # Reset the environment\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "\n",
        "    # repeat\n",
        "    for step in range(max_steps):\n",
        "      # Choose the action At using epsilon greedy policy\n",
        "      action = epsilon_greedy_policy(Qtable, state)\n",
        "\n",
        "      # Take action At and observe Rt+1 and St+1\n",
        "      # Take the action (a) and observe the outcome state(s') and reward (r)\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "\n",
        "      # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n",
        "      Qtable[state][action] = Qtable[state][action] + learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])   \n",
        "\n",
        "      # If done, finish the episode\n",
        "      if done:\n",
        "        break\n",
        "      \n",
        "      # Our state is the new state\n",
        "      state = new_state\n",
        "  return Qtable"
      ],
      "metadata": {
        "id": "CIFJL4nk7_XW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)"
      ],
      "metadata": {
        "id": "Ecg33xPH8CK1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Qtable_frozenlake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPsSy8L28EYj",
        "outputId": "7f8f874b-29fd-4698-a151-a98c79009663"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.73509189, 0.77378094, 0.77378094, 0.73509189],\n",
              "       [0.73509189, 0.        , 0.81450625, 0.77378094],\n",
              "       [0.77378094, 0.857375  , 0.77378094, 0.81450625],\n",
              "       [0.81450625, 0.        , 0.77378094, 0.77378094],\n",
              "       [0.77378094, 0.81450625, 0.        , 0.73509189],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.9025    , 0.        , 0.81450625],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.81450625, 0.        , 0.857375  , 0.77378094],\n",
              "       [0.81450625, 0.9025    , 0.9025    , 0.        ],\n",
              "       [0.857375  , 0.95      , 0.        , 0.857375  ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        ],\n",
              "       [0.        , 0.9025    , 0.95      , 0.857375  ],\n",
              "       [0.9025    , 0.95      , 1.        , 0.9025    ],\n",
              "       [0.        , 0.        , 0.        , 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "  :param Q: The Q-table\n",
        "  :param seed: The evaluation seed array (for taxi-v3)\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in range(n_eval_episodes):\n",
        "    if seed:\n",
        "      state = env.reset(seed=seed[episode])\n",
        "    else:\n",
        "      state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards_ep = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "      # Take the action (index) that have the maximum expected future reward given that state\n",
        "      action = np.argmax(Q[state][:])\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "        \n",
        "      if done:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ],
      "metadata": {
        "id": "7-ugeg2V8GCk"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate our Agent\n",
        "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
        "print(f\"Mean_reward={mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CHtX_E258IyP",
        "outputId": "53f00e45-d132-48ed-e4f6-47bd4ef2fb0c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean_reward=1.00 +/- 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "iayqlYiW8KwZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}