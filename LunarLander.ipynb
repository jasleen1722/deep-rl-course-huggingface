{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LunarLander.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJKn2IrXaLrq"
      },
      "outputs": [],
      "source": [
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay\n",
        "\n",
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[box2d]                 #contains the LunarLander env\n",
        "!pip install stable-baselines3[extra]   #DRL lib\n",
        "!pip install huggingface_sb3\n",
        "!pip install pyglet\n",
        "!pip install ale-py==0.7.4 # To overcome an issue with gym (https://github.com/DLR-RM/stable-baselines3/issues/875)"
      ],
      "metadata": {
        "id": "zB7cYCtHai2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "# from huggingface_sb3 import load_from_hub, package_to_hub, push_to_hub\n",
        "# from huggingface_hub import notebook_login \n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.env_util import make_vec_env"
      ],
      "metadata": {
        "id": "4enQvhM1awzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "# First, we create our environment called LunarLander-v2\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "# Then we reset this environment\n",
        "observation = env.reset()\n",
        "\n",
        "for _ in range(20):\n",
        "  # Take a random action\n",
        "  action = env.action_space.sample()\n",
        "  print(\"Action taken:\", action)\n",
        "\n",
        "  # Do this action in the environment and get\n",
        "  # next_state, reward, done and info\n",
        "  observation, reward, done, info = env.step(action)\n",
        "  \n",
        "  # If the game is done (in our case we land, crashed or timeout)\n",
        "  if done:\n",
        "      # Reset the environment\n",
        "      print(\"Environment is reset\")\n",
        "      observation = env.reset()"
      ],
      "metadata": {
        "id": "1Z_hUOV4ayH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "env.reset()\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space Shape\", env.observation_space.shape)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ],
      "metadata": {
        "id": "vRUHsIbZbIiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment (stacking multiple envs)\n",
        "env = make_vec_env('LunarLander-v2', n_envs=16)"
      ],
      "metadata": {
        "id": "r8DjQ6aRbXi0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SB3 PPO"
      ],
      "metadata": {
        "id": "CTVVMq9DbuU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PPO(policy = 'MlpPolicy',\n",
        "    env = env,\n",
        "    n_steps = 1024,\n",
        "    batch_size = 64,\n",
        "    n_epochs = 4,\n",
        "    gamma = 0.999,\n",
        "    gae_lambda = 0.98,\n",
        "    ent_coef = 0.01,\n",
        "    verbose=1)"
      ],
      "metadata": {
        "id": "ccyOYfNNbrCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train it for 100,000 timesteps\n",
        "model.learn(total_timesteps=100000)\n",
        "# Save the model\n",
        "\n",
        "modelname = \"ppo-ll-v2\"\n",
        "model.save(modelname)"
      ],
      "metadata": {
        "id": "zC5kjen2cNst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title\n",
        "eval_env = gym.make('LunarLander-v2')\n",
        "\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kCzLp5L3dTR3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}